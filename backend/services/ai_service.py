# backend/services/ai_service.py - æ–‡æª”ç®¡ç† AI æœå‹™ï¼ˆå®Œæ•´ç‰ˆï¼Œå¯ç›´æ¥æ›¿æ›ï¼‰
# -*- coding: utf-8 -*-

import os
import requests
import sqlite3
from datetime import datetime
from typing import List, Dict, Any, Optional, Tuple
import pandas as pd
from pathlib import Path
import hashlib
import logging
import re
import shutil
import warnings  # â† æ–°å¢ï¼šæŠ‘åˆ¶ç‰¹å®šè­¦å‘Š

# ç²¾æº–æŠ‘åˆ¶ï¼šä¸å½±éŸ¿å…¶ä»– FutureWarning
warnings.filterwarnings(
    "ignore",
    message="`encoder_attention_mask` is deprecated and will be removed in version 4.55.0",
    category=FutureWarning,
)
warnings.filterwarnings(
    "ignore",
    message="Valid config keys have changed in V2",
    category=UserWarning,
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ æª”æ¡ˆ/OCR ä¾è³´ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import PyPDF2
import docx
import openpyxl  # noqa: F401  # for .xlsx
from PIL import Image
import pytesseract  # OCR for images

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ LangChain / å‘é‡åº« â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.prompts import PromptTemplate
from langchain.schema import Document

# Parent-Child / Contextual Compression / HyDE
from langchain.retrievers import ParentDocumentRetriever, ContextualCompressionRetriever
from langchain.storage import InMemoryStore
from langchain.retrievers.document_compressors import (
    EmbeddingsFilter,
    DocumentCompressorPipeline,
)

# OpenAIï¼ˆå¯é¸ï¼‰
try:
    from langchain.retrievers.contextual_compression import LLMChainExtractor
    from langchain_openai import ChatOpenAI
except Exception:
    LLMChainExtractor = None
    ChatOpenAI = None

# FAISS ä½éšæ§‹ä»¶ + Docstoreï¼ˆç‚ºäº†å»ºç«‹ç©ºç´¢å¼•ï¼‰
import faiss
from langchain_community.docstore import InMemoryDocstore

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ åŸºæœ¬è¨­å®š â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("services.ai_service")

# å„²å­˜ç›®éŒ„
DOCUMENTS_DIR = Path("documents")
DOCUMENTS_DIR.mkdir(exist_ok=True)

INDEX_DIR = Path("vectorstore")
INDEX_DIR.mkdir(exist_ok=True)

# æ˜¯å¦å•Ÿç”¨ SQLite FTS5ï¼ˆè‹¥ç’°å¢ƒä¸æ”¯æ´æœƒè‡ªå‹• fallbackï¼‰
USE_FTS = True

# æ”¯æ´å‰¯æª”å
SUPPORTED_EXTENSIONS = {
    ".pdf", ".docx", ".doc", ".txt", ".md",
    ".xlsx", ".xls", ".csv",
    ".png", ".jpg", ".jpeg", ".tiff"
}

# é¡åˆ¥
DOCUMENT_CATEGORIES = {
    "sop": "Standard Operating Procedures",
    "form": "Forms and Templates",
    "manual": "Manuals and Guides",
    "policy": "Policies and Regulations",
    "checklist": "Checklists",
    "other": "Other Documents",
}

# é è¨­ Ollama æ¨¡å‹
DEFAULT_OLLAMA_MODEL = "gpt-oss:20b"  # ä½ å¯æ”¹å› deepseek-r1:14b


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ å·¥å…·é¡ï¼šæ–‡ä»¶/DB/æ“·å–/åˆ‡å¡Š â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class DocumentManager:
    """æ–‡æª”ç®¡ç†ï¼šSQLite + æª”æ¡ˆ CRUD + æ–‡å­—æ“·å– + åˆ‡å¡Š + FTS"""

    def __init__(self, db_path: str = "documents.db"):
        self.db_path = db_path
        self.fts_enabled = False
        self._init_database()

    def _connect(self):
        conn = sqlite3.connect(self.db_path, check_same_thread=False)
        conn.row_factory = sqlite3.Row
        conn.execute("PRAGMA foreign_keys=ON;")
        conn.execute("PRAGMA journal_mode=WAL;")
        conn.execute("PRAGMA synchronous=NORMAL;")
        conn.execute("PRAGMA temp_store=MEMORY;")
        return conn

    def _init_database(self):
        with self._connect() as conn:
            conn.execute("""
            CREATE TABLE IF NOT EXISTS documents (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                filename TEXT NOT NULL,
                original_name TEXT NOT NULL,
                category TEXT NOT NULL,
                file_type TEXT NOT NULL,
                file_size INTEGER,
                file_hash TEXT UNIQUE,
                content_preview TEXT,
                upload_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                uploaded_by TEXT,
                status TEXT DEFAULT 'active',
                tags TEXT,
                description TEXT
            )""")
            conn.execute("""
            CREATE TABLE IF NOT EXISTS document_chunks (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                document_id INTEGER NOT NULL,
                chunk_index INTEGER,
                content TEXT,
                chunk_size INTEGER,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (document_id) REFERENCES documents (id) ON DELETE CASCADE
            )""")
            conn.execute("CREATE INDEX IF NOT EXISTS ix_docs_status_cat ON documents(status, category);")
            conn.execute("CREATE INDEX IF NOT EXISTS ix_docs_hash ON documents(file_hash);")
            conn.execute("CREATE INDEX IF NOT EXISTS ix_chunks_doc ON document_chunks(document_id);")

            if USE_FTS:
                try:
                    conn.execute("""
                    CREATE VIRTUAL TABLE IF NOT EXISTS documents_fts USING fts5(
                        original_name, tags, description, content='',
                        tokenize='unicode61'
                    )""")
                    conn.execute("""
                    CREATE VIRTUAL TABLE IF NOT EXISTS chunks_fts USING fts5(
                        content, document_id UNINDEXED, chunk_id UNINDEXED,
                        tokenize='unicode61'
                    )""")
                    self.fts_enabled = True
                    logger.info("âœ… SQLite FTS5 å·²å•Ÿç”¨")
                except Exception as e:
                    self.fts_enabled = False
                    logger.warning(f"âš ï¸ ç„¡æ³•å»ºç«‹ FTS5ï¼Œå°‡ä½¿ç”¨ LIKE fallbackï¼š{e}")

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ åŸºç¤å·¥å…· â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def get_file_hash(self, file_path: str) -> str:
        h = hashlib.md5()
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                h.update(chunk)
        return h.hexdigest()

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ æ–‡å­—æ“·å– â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def extract_text_from_file(self, file_path: str, file_type: str) -> tuple[str, str]:
        try:
            if file_type == ".pdf":
                return self._extract_from_pdf(file_path)
            elif file_type in [".docx", ".doc"]:
                return self._extract_from_docx(file_path)
            elif file_type in [".txt", ".md"]:
                return self._extract_from_text(file_path)
            elif file_type in [".xlsx", ".xls", ".csv"]:
                return self._extract_from_excel(file_path)
            elif file_type in [".png", ".jpg", ".jpeg", ".tiff"]:
                return self._extract_from_image(file_path)
            else:
                return "", f"Unsupported file type: {file_type}"
        except Exception as e:
            logger.exception(f"Error extracting text: {file_path}")
            return "", f"Error: {str(e)}"

    def _extract_from_pdf(self, file_path: str) -> tuple[str, str]:
        """PDF æŠ½å–ï¼›é é¢æŠ½ä¸åˆ°å­—æ™‚ï¼Œå°å‰ 5 é è©¦å°æˆæœ¬ OCR"""
        try:
            text = ""
            with open(file_path, "rb") as f:
                reader = PyPDF2.PdfReader(f)
                for i, page in enumerate(reader.pages):
                    ptxt = page.extract_text() or ""
                    if not ptxt.strip() and i < 5:
                        try:
                            from pdf2image import convert_from_path  # type: ignore
                            imgs = convert_from_path(file_path, first_page=i+1, last_page=i+1, dpi=200)
                            if imgs:
                                ptxt = pytesseract.image_to_string(imgs[0], lang="eng+chi_tra+chi_sim")
                        except Exception:
                            pass
                    text += ptxt + "\n"
            return text.strip(), ""
        except Exception as e:
            return "", f"PDF extraction error: {str(e)}"

    def _extract_from_docx(self, file_path: str) -> tuple[str, str]:
        try:
            doc = docx.Document(file_path)
            text = "\n".join([p.text for p in doc.paragraphs])
            return text.strip(), ""
        except Exception as e:
            return "", f"DOCX extraction error: {str(e)}"

    def _extract_from_text(self, file_path: str) -> tuple[str, str]:
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                return f.read().strip(), ""
        except UnicodeDecodeError:
            try:
                with open(file_path, "r", encoding="gbk", errors="ignore") as f:
                    return f.read().strip(), ""
            except Exception as e:
                return "", f"Text extraction error: {str(e)}"
        except Exception as e:
            return "", f"Text extraction error: {str(e)}"

    def _extract_from_excel(self, file_path: str) -> tuple[str, str]:
        try:
            if file_path.endswith(".csv"):
                try:
                    df = pd.read_csv(file_path)
                except UnicodeDecodeError:
                    df = pd.read_csv(file_path, encoding="utf-8", errors="ignore")
            else:
                df = pd.read_excel(file_path)
            text = f"File: {os.path.basename(file_path)}\n"
            text += f"Columns: {', '.join(map(str, df.columns.tolist()))}\n"
            text += f"Rows: {len(df)}\n\nContent Preview:\n"
            text += df.head(10).to_string()
            return text, ""
        except Exception as e:
            return "", f"Excel extraction error: {str(e)}"

    def _extract_from_image(self, file_path: str) -> tuple[str, str]:
        try:
            image = Image.open(file_path)
            text = pytesseract.image_to_string(image, lang="eng+chi_tra+chi_sim")
            return text.strip(), ""
        except Exception as e:
            return "", f"OCR extraction error: {str(e)}"

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ä¸Šå‚³ / åˆªé™¤ / è®€å– â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def upload_document(
        self,
        file_path: str,
        original_name: str,
        category: str,
        uploaded_by: str,
        tags: str = "",
        description: str = "",
    ) -> Dict[str, Any]:
        try:
            if not os.path.exists(file_path):
                return {"success": False, "error": "File not found"}

            file_size = os.path.getsize(file_path)
            file_type = Path(file_path).suffix.lower()
            if file_type not in SUPPORTED_EXTENSIONS:
                return {"success": False, "error": f"Unsupported file type: {file_type}"}

            file_hash = self.get_file_hash(file_path)

            with self._connect() as conn:
                exists = conn.execute("SELECT id FROM documents WHERE file_hash=?", (file_hash,)).fetchone()
                if exists:
                    return {"success": False, "error": "Document already exists"}

            content, error = self.extract_text_from_file(file_path, file_type)
            if error and not content:
                return {"success": False, "error": error}

            preview = content[:500] + "..." if len(content) > 500 else content
            if tags:
                tags = ",".join(sorted(set(t.strip().lower() for t in tags.split(",") if t.strip())))

            chunks = self._split_text_into_chunks(content) if content else []
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            safe_name = re.sub(r"[^\w\-_\.]", "_", original_name)
            new_filename = f"{timestamp}_{safe_name}"
            new_file_path = DOCUMENTS_DIR / new_filename

            with self._connect() as conn:
                cur = conn.cursor()
                cur.execute("""
                    INSERT INTO documents
                    (filename, original_name, category, file_type, file_size, file_hash,
                     content_preview, uploaded_by, tags, description)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, (new_filename, original_name, category, file_type, file_size,
                      file_hash, preview, uploaded_by, tags, description))
                document_id = cur.lastrowid

                chunk_ids = []
                for i, ck in enumerate(chunks):
                    cur.execute("""
                        INSERT INTO document_chunks (document_id, chunk_index, content, chunk_size)
                        VALUES (?, ?, ?, ?)
                    """, (document_id, i, ck, len(ck)))
                    chunk_ids.append(cur.lastrowid)

                if self.fts_enabled:
                    cur.execute(
                        "INSERT INTO documents_fts(rowid, original_name, tags, description) VALUES (?, ?, ?, ?)",
                        (document_id, original_name, tags or "", description or ""),
                    )
                    for cid, ctext in zip(chunk_ids, chunks):
                        cur.execute(
                            "INSERT INTO chunks_fts(rowid, content, document_id, chunk_id) VALUES (?, ?, ?, ?)",
                            (cid, ctext, document_id, cid),
                        )

            shutil.copy2(file_path, new_file_path)

            return {
                "success": True,
                "document_id": document_id,
                "filename": new_filename,
                "content_length": len(content),
                "chunks_created": len(chunks),
            }
        except Exception as e:
            logger.exception("Error uploading document")
            return {"success": False, "error": str(e)}

    def _split_text_into_chunks(self, text: str) -> List[str]:
        """ä¸­è‹±é©ç”¨åˆ‡å¡Š"""
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            separators=["\n\n", "\n", ". ", "ã€‚", " ", ""],
        )
        return splitter.split_text(text)

    def get_documents(self, category: str = None, status: str = "active") -> List[Dict[str, Any]]:
        with self._connect() as conn:
            q = "SELECT * FROM documents WHERE status=?"
            p = [status]
            if category:
                q += " AND category=?"
                p.append(category)
            q += " ORDER BY upload_date DESC"
            rows = conn.execute(q, p).fetchall()

        docs = []
        for r in rows:
            docs.append({
                "id": r["id"],
                "filename": r["filename"],
                "original_name": r["original_name"],
                "category": r["category"],
                "file_type": r["file_type"],
                "file_size": r["file_size"],
                "content_preview": r["content_preview"],
                "upload_date": r["upload_date"],
                "uploaded_by": r["uploaded_by"],
                "tags": r["tags"],
                "description": r["description"],
            })
        return docs

    def get_document_by_id(self, document_id: int) -> Optional[Dict[str, Any]]:
        with self._connect() as conn:
            r = conn.execute(
                "SELECT * FROM documents WHERE id=? AND status='active'", (document_id,)
            ).fetchone()
        if not r:
            return None
        return {
            "id": r["id"],
            "filename": r["filename"],
            "original_name": r["original_name"],
            "category": r["category"],
            "file_type": r["file_type"],
            "file_size": r["file_size"],
            "file_hash": r["file_hash"],
            "content_preview": r["content_preview"],
            "upload_date": r["upload_date"],
            "uploaded_by": r["uploaded_by"],
            "tags": r["tags"],
            "description": r["description"],
        }

    def search_documents_by_name(self, search_term: str) -> List[Dict[str, Any]]:
        rows = []
        with self._connect() as conn:
            if self.fts_enabled and search_term.strip():
                try:
                    q = self._fts_safe_query(search_term)
                    # é‡è¦ï¼šMATCH / bm25() ä½¿ç”¨ã€Œè¡¨åã€ï¼Œä¸è¦ç”¨åˆ¥å
                    rows = conn.execute(f"""
                        SELECT d.*
                        FROM documents_fts
                        JOIN documents d ON d.id = documents_fts.rowid
                        WHERE d.status='active' AND documents_fts MATCH '{q}'
                        ORDER BY bm25(documents_fts)
                        LIMIT 50
                    """).fetchall()
                except Exception as e:
                    logger.warning(f"FTS æ–‡ä»¶æŸ¥è©¢å¤±æ•—ï¼Œä½¿ç”¨ LIKEï¼š{e}")
                    rows = []
            if not rows:
                like = f"%{search_term.lower()}%"
                rows = conn.execute("""
                    SELECT * FROM documents
                    WHERE status='active' AND (
                        LOWER(original_name) LIKE ? OR LOWER(tags) LIKE ? OR LOWER(description) LIKE ?
                    )
                    ORDER BY upload_date DESC
                    LIMIT 50
                """, (like, like, like)).fetchall()

        return [{
            "id": r["id"],
            "filename": r["filename"],
            "original_name": r["original_name"],
            "category": r["category"],
            "file_type": r["file_type"],
            "file_size": r["file_size"],
            "upload_date": r["upload_date"],
            "tags": r["tags"],
            "description": r["description"],
        } for r in rows]

    def get_document_path(self, document_id: int) -> Optional[Path]:
        doc = self.get_document_by_id(document_id)
        if doc:
            return DOCUMENTS_DIR / doc["filename"]
        return None

    def delete_document(self, document_id: int) -> bool:
        try:
            with self._connect() as conn:
                r = conn.execute("SELECT filename FROM documents WHERE id=?", (document_id,)).fetchone()
                if not r:
                    return False
                filename = r["filename"]
                conn.execute("DELETE FROM document_chunks WHERE document_id=?", (document_id,))
                conn.execute("DELETE FROM documents WHERE id=?", (document_id,))
                if self.fts_enabled:
                    conn.execute("DELETE FROM documents_fts WHERE rowid=?", (document_id,))
                    conn.execute("DELETE FROM chunks_fts WHERE document_id=?", (document_id,))

            f = DOCUMENTS_DIR / filename
            if f.exists():
                os.remove(f)
            return True
        except Exception as e:
            logger.exception(f"Error deleting document {document_id}")
            return False

    # å–æ•´ä»½æ–‡ä»¶å…§å®¹ï¼ˆä»¥ chunk åˆä½µï¼‰
    def get_full_text_by_document_id(self, document_id: int) -> str:
        with self._connect() as conn:
            rows = conn.execute("""
                SELECT content FROM document_chunks
                WHERE document_id=?
                ORDER BY chunk_index ASC
            """, (document_id,)).fetchall()
        return "\n".join([r["content"] for r in rows]) if rows else ""

    # æä¾›çµ¦ FTS æ¸…æ´—ï¼ˆè‹±æ–‡åŠ  *ï¼Œä¸­æ–‡ä¿ç•™ï¼‰
    def _fts_safe_query(self, text: str) -> str:
        toks = re.findall(r"[A-Za-z0-9\u4e00-\u9fff]+", text.lower())
        norm = []
        for t in toks:
            if re.match(r"^[a-z0-9]+$", t):
                norm.append(f"{t}*")
            else:
                norm.append(t)
        return " ".join(norm) if norm else text.strip()


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ RAG æ ¸å¿ƒï¼šFAISS + Parent-Child + HyDE + å£“ç¸® â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class DocumentRAG:
    """
    - ä¸» FAISSï¼šä»¥ DB chunks å»º indexï¼ˆMMRï¼‰
    - Parent-Childï¼šä½¿ç”¨ ParentDocumentRetrieverï¼ˆchildï¼šå°å¡Šï¼Œparentï¼šå¤§æ®µï¼‰
    - HyDEï¼šåˆæˆå‡æƒ³ç­”æ¡ˆå†æª¢ç´¢
    - Contextual Compressionï¼šEmbeddingsFilterï¼ˆå¯ç–Š LLMChainExtractorï¼‰
    """

    def __init__(self, document_manager: DocumentManager):
        self.dm = document_manager
        self.embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

        # ä¸»å‘é‡åº«ï¼ˆchunksï¼‰
        self.vectorstore: Optional[FAISS] = None

        # Parent-Child çµæ§‹
        self.parent_store = InMemoryStore()   # åƒ…è¨˜æ†¶é«”ï¼ˆå•Ÿå‹•æ™‚é‡å»ºï¼‰
        self.pc_vectorstore: Optional[FAISS] = None
        self.parent_splitter = RecursiveCharacterTextSplitter(
            chunk_size=3000, chunk_overlap=200, separators=["\n\n", "\n", ". ", "ã€‚", " ", ""]
        )
        self.child_splitter = RecursiveCharacterTextSplitter(
            chunk_size=800, chunk_overlap=120, separators=["\n\n", "\n", ". ", "ã€‚", " ", ""]
        )
        self.parent_retriever: Optional[ParentDocumentRetriever] = None

        # åˆå§‹åŒ–
        self._load_or_build()

    # å»ºç«‹ç©ºçš„ FAISSï¼ˆæ­£ç¢ºç¶­åº¦ï¼‰ä¾› Parent-Child ä½¿ç”¨
    def _new_empty_faiss(self) -> FAISS:
        dim = len(self.embedding_model.embed_query("dimension probe"))
        index = faiss.IndexFlatL2(dim)
        return FAISS(
            embedding_function=self.embedding_model,
            index=index,
            docstore=InMemoryDocstore(),
            index_to_docstore_id={}
        )

    # â”€â”€â”€â”€â”€ ä¸» FAISS â”€â”€â”€â”€â”€
    def _load_or_build(self):
        try:
            faiss_path = INDEX_DIR / "index.faiss"
            if faiss_path.exists():
                self.vectorstore = FAISS.load_local(
                    str(INDEX_DIR), self.embedding_model, allow_dangerous_deserialization=True
                )
                logger.info("âœ… å·²å¾ç£ç¢Ÿè¼‰å…¥ä¸» FAISS")
            else:
                self._build_vector_store()
                self._save()
        except Exception as e:
            logger.error(f"FAISS è¼‰å…¥å¤±æ•—ï¼Œæ”¹ç‚ºé‡å»ºï¼š{e}")
            self._build_vector_store()
            self._save()

        # Parent-Child çµæ§‹æ¯æ¬¡å•Ÿå‹•éƒ½é‡å»ºï¼ˆInMemoryStoreï¼‰
        try:
            self._build_parent_child_index()
            logger.info("âœ… Parent-Child çµæ§‹å·²å»ºç«‹")
        except Exception as e:
            logger.warning(f"âš ï¸ Parent-Child å»ºç«‹å¤±æ•—ï¼š{e}")

    def _save(self):
        if self.vectorstore:
            self.vectorstore.save_local(str(INDEX_DIR))
        # parent-child çš„ docstore ç‚ºè¨˜æ†¶é«”ï¼Œä¸åšæŒä¹…åŒ–

    def _build_vector_store(self, exclude_doc_ids: Optional[set[int]] = None):
        exclude_doc_ids = exclude_doc_ids or set()
        with self.dm._connect() as conn:
            rows = conn.execute("""
                SELECT dc.id AS chunk_id, dc.content, d.original_name, d.category, d.id AS doc_id, d.tags, d.description
                FROM document_chunks dc
                JOIN documents d ON dc.document_id = d.id
                WHERE d.status='active'
            """).fetchall()

        docs: List[Document] = []
        for r in rows:
            if r["doc_id"] in exclude_doc_ids:
                continue
            docs.append(Document(
                page_content=r["content"],
                metadata={
                    "document_id": r["doc_id"],
                    "chunk_id": r["chunk_id"],
                    "filename": r["original_name"],
                    "category": r["category"],
                    "tags": r["tags"] or "",
                    "description": r["description"] or "",
                },
            ))
        if docs:
            self.vectorstore = FAISS.from_documents(docs, self.embedding_model)
            logger.info(f"âœ… ä¸» FAISS é‡å»ºå®Œæˆï¼Œchunks={len(docs)}")
        else:
            self.vectorstore = None
            logger.warning("âš ï¸ ç„¡å¯ç”¨æ–‡ä»¶å»ºç«‹ä¸»å‘é‡åº«")

    def rebuild_vector_store(self):
        self._build_vector_store()
        self._save()

    def add_document_chunks(self, document_id: int):
        """å¢é‡è¿½åŠ å–®ä¸€æª”æ¡ˆ chunks è‡³ä¸» FAISSï¼Œä¸¦åŒæ­¥åˆ° Parent-Child"""
        # ä¸» FAISS
        with self.dm._connect() as conn:
            rows = conn.execute("""
                SELECT dc.id AS chunk_id, dc.content, d.original_name, d.category, d.id AS doc_id, d.tags, d.description
                FROM document_chunks dc
                JOIN documents d ON dc.document_id = d.id
                WHERE d.status='active' AND d.id=?
            """, (document_id,)).fetchall()
        docs = [Document(
            page_content=r["content"],
            metadata={
                "document_id": r["doc_id"],
                "chunk_id": r["chunk_id"],
                "filename": r["original_name"],
                "category": r["category"],
                "tags": r["tags"] or "",
                "description": r["description"] or "",
            },
        ) for r in rows]
        if docs:
            if not self.vectorstore:
                self.vectorstore = FAISS.from_documents(docs, self.embedding_model)
            else:
                self.vectorstore.add_documents(docs)
            self._save()

        # Parent-Childï¼šé‡å°è©²æ–‡ä»¶å¢é‡åŠ å…¥ï¼ˆä¸å‚³ idsï¼Œé¿å…é•·åº¦ä¸ä¸€è‡´ï¼‰
        try:
            self._add_parent_child_for_doc(document_id)
        except Exception as e:
            logger.warning(f"Parent-Child å¢é‡åŠ å…¥å¤±æ•—ï¼ˆdoc={document_id}ï¼‰ï¼š{e}")

    def remove_document(self, document_id: int):
        """FAISS ç„¡æ³•ç²¾æº–åˆªé™¤ docï¼Œæ”¹ç‚ºé‡å»ºï¼ˆæ’é™¤æ­¤ docï¼‰"""
        logger.info(f"ğŸ§¹ é‡æ–°å»ºç«‹ä¸»å‘é‡åº«ï¼ˆæ’é™¤æ–‡ä»¶ {document_id}ï¼‰")
        self._build_vector_store(exclude_doc_ids={document_id})
        self._save()

        # Parent-Childï¼šå…¨é‡å»ºï¼ˆç°¡åŒ–ï¼‰
        try:
            self._build_parent_child_index()
        except Exception as e:
            logger.warning(f"Parent-Child é‡å»ºå¤±æ•—ï¼š{e}")

    # â”€â”€â”€â”€â”€ Parent-Child â”€â”€â”€â”€â”€
    def _build_parent_child_index(self):
        """ä»¥ InMemoryStore + ç©º FAISSï¼ˆæ­£ç¢ºç¶­åº¦ï¼‰é‡æ–°å»ºç«‹ Parent-Child ç´¢å¼•"""
        self.parent_store = InMemoryStore()
        self.pc_vectorstore = self._new_empty_faiss()
        self.parent_retriever = ParentDocumentRetriever(
            vectorstore=self.pc_vectorstore,
            docstore=self.parent_store,
            child_splitter=self.child_splitter,
            parent_splitter=self.parent_splitter,
        )

        # æŠŠæ¯ä¸€ä»½ document çš„å…¨æ–‡ï¼ˆç”± chunks åˆä½µï¼‰ç•¶æˆ parent åŠ å…¥
        docs = self.dm.get_documents()
        parent_docs: List[Document] = []
        for d in docs:
            full_text = self.dm.get_full_text_by_document_id(d["id"])
            if not full_text.strip():
                continue
            parent_docs.append(Document(
                page_content=full_text,
                metadata={
                    "document_id": d["id"],
                    "filename": d["original_name"],
                    "category": d["category"],
                    "tags": d.get("tags") or "",
                    "description": d.get("description") or "",
                },
            ))
        if parent_docs:
            # é—œéµï¼šä¸è¦å‚³ idsï¼Œè®“æª¢ç´¢å™¨è‡ªè¡Œç”Ÿæˆï¼Œé¿å…ã€Œuneven list of documents and idsã€
            self.parent_retriever.add_documents(parent_docs)
        logger.info(f"âœ… Parent-Child ç´¢å¼•å»ºç«‹å®Œæˆï¼Œparents={len(parent_docs)}")

    def _add_parent_child_for_doc(self, document_id: int):
        """æ–°å¢å–®ä¸€æ–‡ä»¶è‡³ Parent-Childï¼ˆä¸å‚³ idsï¼‰"""
        if not self.parent_retriever:
            self._build_parent_child_index()
            return
        meta = self.dm.get_document_by_id(document_id)
        if not meta:
            return
        full_text = self.dm.get_full_text_by_document_id(document_id)
        if not full_text.strip():
            return
        pd = Document(
            page_content=full_text,
            metadata={
                "document_id": document_id,
                "filename": meta["original_name"],
                "category": meta["category"],
                "tags": meta.get("tags") or "",
                "description": meta.get("description") or "",
            },
        )
        self.parent_retriever.add_documents([pd])  # ä¸å‚³ ids

    # â”€â”€â”€â”€â”€ FTS5 å€™é¸ï¼ˆå®‰å…¨å…§æ’ï¼šMATCH/bm25 ä½¿ç”¨è¡¨åï¼‰ â”€â”€â”€â”€â”€
    def _fts_candidates(self, query: str, limit: int = 30) -> List[sqlite3.Row]:
        if not self.dm.fts_enabled or not query.strip():
            return []
        q = self.dm._fts_safe_query(query)
        try:
            with self.dm._connect() as conn:
                rows = conn.execute(f"""
                    SELECT chunks_fts.rowid AS chunk_rowid,
                           chunks_fts.content,
                           chunks_fts.document_id,
                           chunks_fts.chunk_id,
                           d.original_name,
                           d.category,
                           d.tags,
                           d.description
                    FROM chunks_fts
                    JOIN documents d ON d.id = chunks_fts.document_id
                    WHERE d.status='active'
                      AND chunks_fts MATCH '{q}'
                    ORDER BY bm25(chunks_fts)
                    LIMIT {int(limit)}
                """).fetchall()
            return rows or []
        except Exception as e:
            logger.warning(f"FTS5 æŸ¥è©¢å¤±æ•—ï¼Œfallback LIKEï¼š{e}")
            like = f"%{query.lower()}%"
            with self.dm._connect() as conn:
                rows = conn.execute("""
                    SELECT dc.id   AS chunk_rowid,
                           dc.content,
                           d.id    AS document_id,
                           dc.id   AS chunk_id,
                           d.original_name,
                           d.category,
                           d.tags,
                           d.description
                    FROM document_chunks dc
                    JOIN documents d ON d.id = dc.document_id
                    WHERE d.status='active'
                      AND (LOWER(dc.content) LIKE ? OR LOWER(d.original_name) LIKE ? OR LOWER(d.tags) LIKE ?)
                    LIMIT ?
                """, (like, like, like, limit)).fetchall()
            return rows or []

    # â”€â”€â”€â”€â”€ å£“ç¸®æª¢ç´¢å™¨ â”€â”€â”€â”€â”€
    def _build_compression_retriever(
        self,
        base_retriever,
        use_openai: bool = False,
        openai_model: str = "gpt-4o-mini",
        similarity_threshold: float = 0.3,
    ):
        emb_filter = EmbeddingsFilter(
            embeddings=self.embedding_model,
            similarity_threshold=similarity_threshold,
        )

        base_compressor = emb_filter
        if use_openai and ChatOpenAI and LLMChainExtractor and os.environ.get("OPENAI_API_KEY"):
            try:
                llm = ChatOpenAI(model=openai_model, temperature=0)
                extractor = LLMChainExtractor.from_llm(llm)
                base_compressor = DocumentCompressorPipeline(transformers=[emb_filter, extractor])
            except Exception as e:
                logger.warning(f"LLMChainExtractor ç„¡æ³•å»ºç«‹ï¼Œæ”¹ç”¨ EmbeddingsFilterï¼š{e}")
                base_compressor = emb_filter

        return ContextualCompressionRetriever(
            base_compressor=base_compressor,
            base_retriever=base_retriever,
        )

    # â”€â”€â”€â”€â”€ HyDEï¼šåˆæˆç­”æ¡ˆå†æª¢ç´¢ â”€â”€â”€â”€â”€
    def _hyde_generate(self, question: str, use_openai: bool, openai_model: str) -> str:
        tmpl = (
            "You are a helpful assistant. Create a concise, factual, step-by-step hypothetical answer "
            "to the user's question below. Avoid placeholders. Keep it within 120~180 words.\n\n"
            f"QUESTION:\n{question}\n\nHYPOTHETICAL ANSWER:\n"
        )
        try:
            if use_openai and os.environ.get("OPENAI_API_KEY") and ChatOpenAI:
                llm = ChatOpenAI(model=openai_model, temperature=0.1, max_tokens=220)
                out = llm.invoke(tmpl)
                if hasattr(out, "content"):
                    return str(out.content).strip()
                return str(out).strip()
            # fallback to Ollama
            return ollama_generate(tmpl, model_name=DEFAULT_OLLAMA_MODEL)
        except Exception as e:
            logger.warning(f"HyDE ç”Ÿæˆå¤±æ•—ï¼Œå¿½ç•¥ HyDEï¼š{e}")
            return ""

    # â”€â”€â”€â”€â”€ æŸ¥è©¢ä¸»æµç¨‹ â”€â”€â”€â”€â”€
    def query_documents(
        self,
        question: str,
        k: int = 5,
        use_hyde: bool = True,
        use_parent: bool = True,
        use_compression: bool = True,
        use_openai: bool = False,
        openai_model: str = "gpt-4o-mini",
    ) -> List[Document]:
        if not self.vectorstore:
            return []

        # 1) ä¸» MMR æª¢ç´¢
        base_retriever = self.vectorstore.as_retriever(
            search_type="mmr",
            search_kwargs={"k": max(k, 5), "fetch_k": 40, "lambda_mult": 0.5},
        )
        base_docs: List[Document] = base_retriever.invoke(question)

        # 2) FTS å€™é¸ï¼ˆè£œæ¼ï¼‰ï¼Œä¸¦åˆä½µå»é‡
        def _key(d: Document) -> Tuple:
            return (d.metadata.get("document_id"), d.metadata.get("chunk_id"), d.page_content[:50])

        uniq = { _key(d): d for d in base_docs }

        try:
            fts_rows = self._fts_candidates(question, limit=30)
            for r in fts_rows:
                d = Document(
                    page_content=r["content"],
                    metadata={
                        "document_id": r["document_id"],
                        "chunk_id": r["chunk_id"],
                        "filename": r["original_name"],
                        "category": r["category"],
                        "tags": r["tags"] or "",
                        "description": r["description"] or "",
                    },
                )
                uniq.setdefault(_key(d), d)
        except Exception as e:
            logger.warning(f"FTS å€™é¸åˆä½µå¤±æ•—ï¼š{e}")

        # 3) HyDEï¼šåˆæˆç­”æ¡ˆå†æª¢ç´¢ä¸¦åˆä½µ
        if use_hyde:
            hyde_txt = self._hyde_generate(question, use_openai=use_openai, openai_model=openai_model)
            if hyde_txt:
                try:
                    hyde_docs = base_retriever.invoke(hyde_txt)
                    for d in hyde_docs:
                        uniq.setdefault(_key(d), d)
                except Exception as e:
                    logger.warning(f"HyDE æª¢ç´¢å¤±æ•—ï¼š{e}")

        # 4) Parent-Childï¼šæ“·å–ä¸Šä½æ®µè½ï¼ˆæ›´å®Œæ•´ä¸Šä¸‹æ–‡ï¼‰
        if use_parent and self.parent_retriever:
            try:
                parent_docs = self.parent_retriever.invoke(question)
                for pd in parent_docs:
                    pd.metadata.setdefault("document_id", pd.metadata.get("document_id"))
                    pd.metadata.setdefault("chunk_id", f"parent-{hash(pd.page_content) & 0xffff}")
                    uniq.setdefault(_key(pd), pd)
            except Exception as e:
                logger.warning(f"Parent-Child æª¢ç´¢å¤±æ•—ï¼š{e}")

        merged = list(uniq.values())

        # 5) Contextual Compressionï¼ˆå¯é¸ï¼‰
        if use_compression:
            try:
                comp = self._build_compression_retriever(
                    base_retriever=base_retriever,
                    use_openai=use_openai,
                    openai_model=openai_model,
                    similarity_threshold=0.30,
                )
                compressed_docs = comp.invoke(question)
                cuniq = { _key(d): d for d in compressed_docs }
                for d in merged:
                    cuniq.setdefault(_key(d), d)
                merged = list(cuniq.values())
            except Exception as e:
                logger.warning(f"Compression æª¢ç´¢å¤±æ•—ï¼Œä½¿ç”¨æœªå£“ç¸®çµæœï¼š{e}")

        # 6) æ’åºæ“·å–
        merged.sort(key=lambda d: (len(d.page_content), d.metadata.get("category", "")), reverse=True)
        final_docs = merged[:max(k, 8)]
        return final_docs


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ç”Ÿæˆæ¨¡å‹ï¼ˆOllama / OpenAIï¼‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _strip_thinking(text: str) -> str:
    cleaned = re.sub(r"<think>.*?</think>", "", text, flags=re.DOTALL)
    lines = [ln for ln in cleaned.splitlines() if not ln.strip().startswith(("Thinking:", "Processing:", "Analyzing:"))]
    return "\n".join(lines).strip()


def ollama_generate(prompt: str, model_name: str = DEFAULT_OLLAMA_MODEL, stream: bool = False) -> str:
    """æœ¬åœ° Ollama ç”Ÿæˆï¼›num_ctx è¨­ 8192ã€keep_alive 15m"""
    try:
        payload = {
            "model": model_name,
            "prompt": prompt,
            "stream": stream,
            "options": {
                "temperature": 0.2,
                "top_p": 0.1,
                "top_k": 10,
                "repeat_penalty": 1.1,
                "num_predict": 2000,
                "num_ctx": 8192,
            },
            "keep_alive": "15m",
        }
        r = requests.post("http://localhost:11434/api/generate", json=payload, timeout=120)
        r.raise_for_status()
        result = r.json()
        return _strip_thinking(result.get("response", ""))
    except requests.exceptions.RequestException as e:
        raise Exception(f"Ollama é€£ç·šéŒ¯èª¤: {str(e)}")
    except Exception as e:
        raise Exception(f"Ollama å›æ‡‰è™•ç†éŒ¯èª¤: {str(e)}")


def openai_generate(prompt: str, model_name: str = "gpt-4o-mini") -> str:
    """OpenAI Chat Completions"""
    try:
        api_key = os.environ.get("OPENAI_API_KEY")
        if not api_key:
            raise Exception("OpenAI API key not found. Please set OPENAI_API_KEY environment variable.")

        headers = {"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"}
        payload = {
            "model": model_name,
            "messages": [
                {
                    "role": "system",
                    "content": (
                        "You are a helpful assistant specializing in document analysis, SOP guidance, and "
                        "process improvement. Provide clear, actionable responses based on the provided documents."
                    ),
                },
                {"role": "user", "content": prompt},
            ],
            "max_tokens": 2000,
            "temperature": 0.2,
        }
        r = requests.post("https://api.openai.com/v1/chat/completions", headers=headers, json=payload, timeout=60)
        if r.status_code == 200:
            data = r.json()
            return data["choices"][0]["message"]["content"].strip()
        else:
            detail = r.json().get("error", {}).get("message", r.text)
            raise Exception(f"OpenAI API error: {r.status_code} - {detail}")
    except requests.exceptions.RequestException as e:
        raise Exception(f"Error connecting to OpenAI API: {str(e)}")
    except Exception as e:
        raise Exception(f"Error processing OpenAI request: {str(e)}")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Prompt æ¨¡æ¿ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
PROMPT_TEMPLATES = {
    "sop_query": PromptTemplate(
        input_variables=["context", "question"],
        template="""Based on the following SOP documents and procedures, please answer the question:

DOCUMENTS:
{context}

QUESTION: {question}

Please provide a clear, step-by-step answer based on the SOP content. If the question involves process improvement, also suggest potential improvements."""
    ),
    "form_assistance": PromptTemplate(
        input_variables=["context", "question"],
        template="""Based on the following forms and templates, please help with the question:

DOCUMENTS:
{context}

QUESTION: {question}

Provide clear guidance on how to fill out or use these forms correctly."""
    ),
    "improvement_analysis": PromptTemplate(
        input_variables=["context", "question"],
        template="""Analyze the following documents for improvement opportunities:

DOCUMENTS:
{context}

QUESTION: {question}

Focus on:
1. Identifying inefficiencies or gaps
2. Suggesting practical improvements
3. Considering cost-effectiveness and feasibility
4. Ensuring compliance and safety"""
    ),
    "document_request": PromptTemplate(
        input_variables=["context", "question", "found_documents"],
        template="""The user is requesting a specific document. Based on the search results:

FOUND DOCUMENTS:
{found_documents}

RELATED CONTEXT:
{context}

USER REQUEST: {question}

Please help identify which document(s) the user needs and provide guidance on accessing them."""
    ),
    "general_document": PromptTemplate(
        input_variables=["context", "question"],
        template="""Based on the following documents, please answer the question:

DOCUMENTS:
{context}

QUESTION: {question}

Provide a comprehensive answer based on the document content."""
    ),
}

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ æŸ¥è©¢é¡å‹åˆ¤æ–· / åç¨±è§£æ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def determine_query_type(question: str) -> tuple[str, bool]:
    q = question.lower()
    doc_req_kw = ['æˆ‘éœ€è¦', 'i need', 'çµ¦æˆ‘', 'give me', 'ä¸‹è¼‰', 'download',
                  'å°å‡º', 'export', 'æ‰¾åˆ°', 'find', 'æä¾›', 'provide',
                  'ç™¼é€', 'send', 'å–å¾—', 'get', 'è¦', 'want']
    doc_type_kw = ['æ–‡ä»¶', 'æ–‡æª”', 'è¡¨æ ¼', 'è¡¨å–®', 'form', 'document',
                   'file', 'template', 'æ¨¡æ¿', 'sop', 'ç¨‹åº']
    is_doc_req = any(k in q for k in doc_req_kw) and any(k in q for k in doc_type_kw)
    if is_doc_req:
        return "document_request", True

    sop_kw = ['sop', 'procedure', 'process', 'step', 'how to', 'æµç¨‹', 'ç¨‹åº', 'æ­¥é©Ÿ']
    form_kw = ['form', 'template', 'fill', 'complete', 'è¡¨æ ¼', 'å¡«å¯«', 'æ¨¡æ¿']
    improve_kw = ['improve', 'optimize', 'better', 'enhance', 'suggestion', 'æ”¹é€²', 'å„ªåŒ–', 'å»ºè­°']

    if any(k in q for k in improve_kw):
        return "improvement_analysis", False
    elif any(k in q for k in sop_kw):
        return "sop_query", False
    elif any(k in q for k in form_kw):
        return "form_assistance", False
    else:
        return "general_document", False


def extract_document_name_from_question(question: str) -> str:
    q = question.strip()
    m = re.search(r'["â€œâ€](.+?)["â€œâ€]', q)
    if m:
        return m.group(1).strip()
    m = re.search(r'[ï¼ˆ(](.+?)[)ï¼‰]', q)
    if m:
        return m.group(1).strip()

    remove_words = ['æˆ‘éœ€è¦', 'çµ¦æˆ‘', 'ä¸‹è¼‰', 'å°å‡º', 'æ‰¾åˆ°', 'æä¾›', 'ç™¼é€', 'å–å¾—', 'è¦',
                    'i need', 'give me', 'download', 'export', 'find', 'provide', 'send', 'get', 'want',
                    'é€™å€‹', 'é‚£å€‹', 'çš„', 'this', 'that', 'the', 'a', 'an']
    lower_q = q.lower()
    for w in remove_words:
        lower_q = lower_q.replace(w, ' ')
    keywords = ['æ–‡ä»¶', 'æ–‡æª”', 'è¡¨æ ¼', 'è¡¨å–®', 'form', 'document', 'file', 'template', 'æ¨¡æ¿',
                'sop', 'ç¨‹åº', 'checklist', 'æ¸…å–®', 'manual', 'æ‰‹å†Š', 'policy', 'æ”¿ç­–']
    for kw in keywords:
        if kw in lower_q:
            parts = lower_q.split(kw, 1)
            if len(parts) > 1 and parts[1].strip():
                return parts[1].strip()
    return " ".join(lower_q.split())


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Orchestratorï¼šAI æ–‡æª”åˆ†æ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class AIDocumentAnalytics:
    def __init__(self):
        self.document_manager = DocumentManager()
        self.rag_system = DocumentRAG(self.document_manager)
        self.prompt_templates = PROMPT_TEMPLATES
        logger.info("âœ… AI Document Analytics initialized")

    def upload_document(
        self, file_path: str, original_name: str, category: str,
        uploaded_by: str, tags: str = "", description: str = ""
    ) -> Dict[str, Any]:
        result = self.document_manager.upload_document(
            file_path, original_name, category, uploaded_by, tags, description
        )
        if result.get("success"):
            self.rag_system.add_document_chunks(result["document_id"])
            logger.info(f"âœ… Document uploaded & indexes updated: {original_name}")
        return result

    def query_documents(
        self, question: str, use_openai: bool = False, openai_model: str = "gpt-4o-mini"
    ) -> Dict[str, Any]:
        try:
            query_type, is_doc_req = determine_query_type(question)

            # æ–‡ä»¶è«‹æ±‚å„ªå…ˆä»¥åç¨±æœå°‹
            if is_doc_req:
                search_term = extract_document_name_from_question(question)
                found = self.document_manager.search_documents_by_name(search_term)
                if found:
                    first = found[0]
                    resp = "âœ… æ‰¾åˆ°äº†æ‚¨éœ€è¦çš„æ–‡æª”ï¼š\n\n"
                    resp += f"**æ–‡æª”åç¨±**: {first['original_name']}\n"
                    resp += f"**é¡åˆ¥**: {DOCUMENT_CATEGORIES.get(first['category'], first['category'])}\n"
                    resp += f"**æ–‡ä»¶é¡å‹**: {first['file_type'].upper()}\n"
                    if first.get('description'):
                        resp += f"**æè¿°**: {first['description']}\n"
                    if first.get('tags'):
                        resp += f"**æ¨™ç±¤**: {first['tags']}\n"
                    resp += f"\nğŸ“¥ **ä¸‹è¼‰æ–‡æª”**: [ä¸‹è¼‰ {first['original_name']}](/api/ai/documents/{first['id']}/download)\n"

                    if len(found) > 1:
                        resp += f"\nğŸ“š é‚„æ‰¾åˆ°å…¶ä»– {len(found)-1} å€‹ç›¸é—œæ–‡æª”ï¼š\n"
                        for doc in found[1:4]:
                            resp += f"- [{doc['original_name']}](/api/ai/documents/{doc['id']}/download) ({doc['file_type'].upper()})\n"

                    return {
                        "answer": resp,
                        "source_documents": [{
                            "filename": d['original_name'],
                            "category": d['category'],
                            "document_id": d['id'],
                            "content_preview": f"Document type: {d['file_type'].upper()}",
                        } for d in found[:5]],
                        "status": "success",
                        "ai_provider": "system",
                        "query_type": "document_request",
                        "documents_found": found,
                    }
                else:
                    query_type = "general_document"

            # RAGï¼šMMR + FTS + HyDE + Parent-Child + å£“ç¸®
            relevant = self.rag_system.query_documents(
                question=question,
                k=8,
                use_hyde=True,
                use_parent=True,
                use_compression=True,
                use_openai=use_openai,
                openai_model=openai_model,
            )
            if not relevant:
                return {
                    "answer": "æŠ±æ­‰ï¼Œæ‰¾ä¸åˆ°ç›¸é—œæ–‡æª”å…§å®¹å¯ç”¨æ–¼å›ç­”ã€‚è«‹ç¢ºèªå·²ä¸Šå‚³ç›¸é—œæ–‡ä»¶ï¼Œæˆ–æ›´æ›é—œéµè©å†è©¦ã€‚",
                    "source_documents": [],
                    "status": "no_documents",
                }

            # æ§‹å»ºä¸Šä¸‹æ–‡
            context = "\n\n".join([
                f"Document: {d.metadata.get('filename')}\nCategory: {d.metadata.get('category')}\nContent: {d.page_content}"
                for d in relevant
            ])

            tmpl = self.prompt_templates.get(query_type, self.prompt_templates["general_document"])
            prompt = tmpl.format(context=context, question=question) if query_type != "document_request" else tmpl.format(
                context=context, question=question, found_documents=""
            )

            if use_openai:
                answer = openai_generate(prompt, openai_model)
                provider = "openai"
            else:
                answer = ollama_generate(prompt, model_name=DEFAULT_OLLAMA_MODEL)
                provider = "ollama"

            # å›å‚³ä¾†æºæ‘˜è¦
            src = [{
                "filename": d.metadata.get("filename"),
                "category": d.metadata.get("category"),
                "document_id": d.metadata.get("document_id"),
                "content_preview": (d.page_content[:200] + "...") if len(d.page_content) > 200 else d.page_content,
            } for d in relevant]

            return {
                "answer": answer,
                "source_documents": src,
                "status": "success",
                "ai_provider": provider,
                "query_type": query_type,
            }
        except Exception as e:
            logger.exception("Query processing error")
            return {"answer": f"è™•ç†æŸ¥è©¢æ™‚å‡ºéŒ¯: {str(e)}", "source_documents": [], "status": "error"}

    def get_documents(self, category: str = None) -> List[Dict[str, Any]]:
        return self.document_manager.get_documents(category)

    def get_document_by_id(self, document_id: int) -> Optional[Dict[str, Any]]:
        return self.document_manager.get_document_by_id(document_id)

    def get_document_path(self, document_id: int) -> Optional[Path]:
        return self.document_manager.get_document_path(document_id)

    def delete_document(self, document_id: int) -> bool:
        success = self.document_manager.delete_document(document_id)
        if success:
            self.rag_system.remove_document(document_id)
            logger.info(f"âœ… Document deleted & indexes rebuilt(excluded): {document_id}")
        return success

    def get_categories(self) -> Dict[str, str]:
        return DOCUMENT_CATEGORIES

    def get_system_status(self) -> Dict[str, Any]:
        try:
            docs = self.document_manager.get_documents()
            with self.document_manager._connect() as conn:
                chunk_count = conn.execute("SELECT COUNT(*) AS c FROM document_chunks").fetchone()["c"]
            faiss_path = INDEX_DIR / "index.faiss"
            index_ready = self.rag_system.vectorstore is not None
            index_size = faiss_path.stat().st_size if faiss_path.exists() else 0

            cat = {}
            for d in docs:
                cat[d["category"]] = cat.get(d["category"], 0) + 1

            return {
                "status": "ready",
                "total_documents": len(docs),
                "total_chunks": int(chunk_count),
                "categories": cat,
                "vector_store_ready": index_ready,
                "vector_store_size_bytes": index_size,
                "supported_file_types": sorted(list(SUPPORTED_EXTENSIONS)),
                "ollama_model": DEFAULT_OLLAMA_MODEL,
                "openai_available": bool(os.environ.get("OPENAI_API_KEY")),
                "fts_enabled": self.document_manager.fts_enabled,
                "parent_child_enabled": self.rag_system.parent_retriever is not None,
            }
        except Exception as e:
            logger.exception("Error getting system status")
            return {"status": "error", "message": str(e)}


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ å…¨å±€ AI å¼•æ“å¯¦ä¾‹ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ai_engine = AIDocumentAnalytics()
